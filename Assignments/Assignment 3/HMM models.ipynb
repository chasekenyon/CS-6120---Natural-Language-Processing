{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this tutorial, we are going to perform Part-of-Speech (POS) tagging using Hidden Markov Models (HMM) and the Viterbi algorithm. POS tagging is the task of labeling the words in a sentence with their appropriate Part of Speech.\n",
    "\n",
    "**Hidden Markov Models (HMMs)** are statistical models where the system being modeled is assumed to be a Markov process with hidden states. An HMM can be visualized as a directed, weighted graph, where each edge's weight represents the probability of transitioning between two states. You can see a [visual representation of an HMM here](https://en.wikipedia.org/wiki/File:HMMGraph.svg).\n",
    "\n",
    "The **Viterbi algorithm** is a dynamic programming algorithm for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events. It uses the concept of path and state probabilities. You can see a [step-by-step example of the Viterbi algorithm here](https://www.cis.upenn.edu/~cis262/notes/Example-Viterbi-DNA.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "[# Necessary imports for the notebook\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning, we import the necessary libraries for this tutorial. numpy is used for handling operations on large, multi-dimensional arrays and matrices, and defaultdict is a dictionary subclass that provides a default value for the key that does not exists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess the data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Skip empty lines\n",
    "            word, tag = line.split('\\t')\n",
    "            data.append((word, tag))\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = load_data('pos_data/train.pos')\n",
    "test_data = load_data('pos_data/test.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'), ('an', 'DT'), ('Oct.', 'NNP'), ('19', 'CD'), ('review', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of code, we're defining a function **load_data()** that will be used to load and preprocess the dataset. The function reads the file line by line, and for each line, it extracts the word and its corresponding tag, which are appended as a tuple to the data list. The function then returns the data list. We use this function to load our training and test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Emission and Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emission probabilities\n",
    "def emission_probabilities(data):\n",
    "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    for word, tag in data:\n",
    "        emission_counts[tag][word] += 1\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "    # Calculate probabilities\n",
    "    emission_probs = defaultdict(dict)\n",
    "    for tag in emission_counts.keys():\n",
    "        for word in emission_counts[tag].keys():\n",
    "            emission_probs[tag][word] = emission_counts[tag][word] / tag_counts[tag]\n",
    "    \n",
    "    return emission_probs\n",
    "\n",
    "\n",
    "# Calculate transition probabilities\n",
    "def transition_probabilities(data):\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(data) - 1):\n",
    "        current_tag = data[i][1]\n",
    "        next_tag = data[i + 1][1]\n",
    "        transition_counts[current_tag][next_tag] += 1\n",
    "        tag_counts[current_tag] += 1\n",
    "\n",
    "    # Calculate probabilities\n",
    "    transition_probs = defaultdict(dict)\n",
    "    for tag in transition_counts.keys():\n",
    "        for next_tag in transition_counts[tag].keys():\n",
    "            transition_probs[tag][next_tag] = transition_counts[tag][next_tag] / tag_counts[tag]\n",
    "    \n",
    "    return transition_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two functions **emission_probabilities()** and **transition_probabilities()** that calculate the emission and transition probabilities respectively.\n",
    "\n",
    "**Emission probability** is the probability of an observation being generated from a state. In our case, it's the probability of a word given its POS tag. It's calculated by dividing the count of a particular word-tag pair by the count of the tag.\n",
    "\n",
    "**Transition probability** is the probability of transitioning from one state to another. In our case, it's the probability of a particular POS tag given the previous POS tag. It's calculated by dividing the count of a tag following a particular tag by the count of the latter tag.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(words, tags, start_probs, transition_probs, emission_probs):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    # Initialize base cases (t == 0)\n",
    "    for tag in tags:\n",
    "        V[0][tag] = start_probs[tag] * emission_probs[tag].get(words[0], 0)\n",
    "        path[tag] = [tag]\n",
    "\n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(words)):\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "\n",
    "        for current_tag in tags:\n",
    "            (prob, state) = max((V[t-1][previous_tag] * transition_probs[previous_tag].get(current_tag, 0) * emission_probs[current_tag].get(words[t], 0), previous_tag) for previous_tag in tags)\n",
    "            V[t][current_tag] = prob\n",
    "            new_path[current_tag] = path[state] + [current_tag] \n",
    "\n",
    "        path = new_path\n",
    "    \n",
    "    (prob, state) = max((V[-1][current_tag], current_tag) for current_tag in tags)\n",
    "    return (prob, path[state])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the Viterbi algorithm. It's a dynamic programming algorithm used to find the most likely sequence of hidden states, given a sequence of observations. It returns the most probable sequence of tags for a given sequence of words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.891973335768423\n"
     ]
    }
   ],
   "source": [
    "# Get unique words and tags from the training data\n",
    "words = list(set([word for word, tag in train_data]))\n",
    "tags = list(set([tag for word, tag in train_data]))\n",
    "\n",
    "# Compute start, transition, and emission probabilities\n",
    "start_probs = {tag: 1/len(tags) for tag in tags}  # Assume uniform distribution for start probabilities\n",
    "transition_probs = transition_probabilities(train_data)\n",
    "emission_probs = emission_probabilities(train_data)\n",
    "\n",
    "# Implement Viterbi on the test data\n",
    "predicted_tags = []\n",
    "for word, tag in test_data:\n",
    "    prob, path = viterbi([word], tags, start_probs, transition_probs, emission_probs)\n",
    "    predicted_tags.append(path[0])\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_tags = [tag for word, tag in test_data]\n",
    "accuracy = sum([correct == predicted for correct, predicted in zip(correct_tags, predicted_tags)]) / len(correct_tags)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last block of code, we bring all the pieces together.\n",
    "\n",
    "1. Extract unique words and tags from the training data.\n",
    "2. Compute the start, transition, and emission probabilities.\n",
    "3. Implement the Viterbi algorithm on the test data to predict the POS tags.\n",
    "4. Calculate the accuracy of our predictions by comparing with the actual tags.\n",
    "\n",
    "The output of the cell shows the accuracy of our POS tagging, which is approximately 0.89, indicating that our HMM model with the Viterbi algorithm is performing quite well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
