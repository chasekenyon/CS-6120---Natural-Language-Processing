{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "## Transformer code setup from Prof. Uzair Ahmad's Provided Code\n",
    "\n",
    "### Transformer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "''' Look at all previous tokens to generate next\n",
    "    @Author: Uzair Ahmad\n",
    "    2022\n",
    "    +TransformerBlock \n",
    "'''\n",
    "\n",
    "\n",
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
    "                                                              in_size=in_size,\n",
    "                                                              out_size=out_size)\n",
    "            self.think = TransformerBlockLM.MLP(embed_size=out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.think(x + self.comm(x))\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(out_size)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, out_size):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = out_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False)\n",
    "\n",
    "        def forward(self, x):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            autocorrs = torch.tril(autocorrs)\n",
    "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_head_size=8,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # self.device = 'cpu'\n",
    "        \n",
    "        print(f\"Device: {self.device}\") \n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        i = in_ids[:, -self.input_length:].to(self.device)\n",
    "        in_ids_emb = self.token_embeddings_table(i)\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_pos_emb = self.position_embeddings_table(\n",
    "                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)\n",
    "            )\n",
    "            in_ids_emb = in_ids_emb + in_ids_pos_emb\n",
    "\n",
    "        block_outputs = self.blocks(in_ids_emb)\n",
    "        logits = self.linear_sahead_to_vocab(block_outputs)  # compute\n",
    "\n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in range(train_iters):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}:, time = {time.time()-start}, train {avg_loss['train']}, val {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            if inputs is None or targets is None:\n",
    "                continue\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "        # torch.save(self, 'sa_pos_')\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            total_loss = 0.0\n",
    "            total_tokens = 0\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)\n",
    "                if tokens is None or targets is None:\n",
    "                    print(f\"Insufficient data for evaluation, skipping this iteration for {split} split...\")\n",
    "                    continue\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "\n",
    "                total_loss += ce_loss.item() * tokens.size(0)\n",
    "                total_tokens += tokens.size(0)\n",
    "\n",
    "            avg_loss = losses.mean()\n",
    "            perf[split] = avg_loss\n",
    "\n",
    "            # calculate and print perplexity\n",
    "            perplexity = torch.exp(torch.tensor(total_loss / total_tokens).to(self.device)) if total_tokens > 0 else None\n",
    "            print(f\"Perplexity for {split}: {perplexity}\")\n",
    "\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
    "\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long).to(self.device)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long).to(self.device)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size).to(self.device)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size).to(self.device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "        ).to(self.device)\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size).to(self.device)\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        if len(data) - self.input_length < 0:\n",
    "            print(\"Insufficient data for this iteration, skipping...\")\n",
    "            print(f\"Len(data): {len(data)}, input length: {self.input_length}, batch size: {self.batch_size}\")\n",
    "            return None, None\n",
    "        ix = torch.randint(len(data) - self.input_length, (self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device).to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device).to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is code implementing a Transformer model with MLP based positional encoding, code is provided by Dr. Uzair Ahmad.\n",
    "\n",
    "1. The Primary difference between the code provided in the module by Prof. Ahmad and the architecture of the Google Patent is that the code above is using transformer architecture as the neural network used, while Google proposes RNN's. The code above also uses positional encoding from an MLP layer, compared to Google's patent's proposed global attention.\n",
    "\n",
    "2. Corrections above made to be able to run code using CUDA cores, several parts of data required \".to(self.device)\" to properly send to cuda cores, prior to these changes below error was received:\n",
    "\n",
    "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
    "\n",
    "This fixes a bug causing the program to fail to run, while also increasing the speed it can be trained in. \n",
    "\n",
    "In addition, I was frequently having an issue where in validation, the len(data) was less than input length when training on smaller datasets, so I created some checks to avoid this, though the best solution was to simply lower the input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Code\n",
    "\n",
    "Below is a main block to run the transformer on a sample of Emily Dickinson's poems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print(f\"Start: {time.ctime()}\")\n",
    "\n",
    "with open('./Transformer/emily_dickonson.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\n",
    "#        'lazy dog and a quick brown fox.\\n' \\\n",
    "#        'the dog is lazy and the fox jumps quickly.\\n' \\\n",
    "#        'a fox jumps over the dog because he is lazy.\\n' \\\n",
    "#        'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'\n",
    "\n",
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model = model.to(model.device)\n",
    "model.prep(text)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "input_batch, output_batch = model.get_batch(split='train')\n",
    "_, _ = model(input_batch, output_batch)\n",
    "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
    "                                                        dtype=torch.long,\n",
    "                                                        device=model.device),\n",
    "                         max_new_tokens=1000)\n",
    "print(outputs)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Elapsed Time: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "466s on cuda 881s on cpu #sample text\n",
    "\n",
    "481s on cuda 1653s on cpu # emily dickonson\n",
    "\n",
    "**Device:** cpu\n",
    "\n",
    "**params** 1113169\n",
    "\n",
    "**iter 0:** time = 152.3404836654663, train 5.497964382171631, val 5.49129581451416\n",
    "\n",
    "**iter 1000:** time = 562.986720085144, train 1.6596962213516235, val 1.7237656116485596\n",
    "\n",
    "**iter 2000:** time = 971.5997943878174, train 1.3862591981887817, val 1.7082321643829346\n",
    "\n",
    "**iter 3000:** time = 1379.8686068058014, train 1.1676121950149536, val 1.8512464761734009\n",
    "\n",
    "\n",
    "**Device:** cuda\n",
    "\n",
    "**params** 1113169\n",
    "\n",
    "**iter 0:** time = 46.85462760925293, train 5.598533630371094, val 5.547347545623779\n",
    "\n",
    "**iter 1000:** time = 160.57424426078796, train 1.6535813808441162, val 1.7112394571304321\n",
    "\n",
    "**iter 2000:** time = 274.0162696838379, train 1.3842737674713135, val 1.6874533891677856\n",
    "\n",
    "**iter 3000:** time = 387.8321087360382, train 1.1877456903457642, val 1.8289140462875366\n",
    "\n",
    "\n",
    "With the cuda cores being utilized, we can see significant reduction in the time spent training. This can improve our model significantly by limiting the time spent to train, or allowing us to train a more complicated model, or train for longer in the same amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "### Training on Warren Buffet's style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the modified eval_loss method that calculates and prints the perplexity on each step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(self, eval_iters):\n",
    "    perf = {}\n",
    "    self.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            tokens, targets = self.get_batch(split)\n",
    "            if tokens is None or targets is None:\n",
    "                print(f\"Insufficient data for evaluation, skipping this iteration for {split} split...\")\n",
    "                continue\n",
    "            _, ce_loss = self(tokens, targets)  # forward pass\n",
    "            losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "\n",
    "            total_loss += ce_loss.item() * tokens.size(0)\n",
    "            total_tokens += tokens.size(0)\n",
    "\n",
    "        avg_loss = losses.mean()\n",
    "        perf[split] = avg_loss\n",
    "\n",
    "        # calculate and print perplexity\n",
    "        perplexity = torch.exp(torch.tensor(total_loss / total_tokens).to(self.device)) if total_tokens > 0 else None\n",
    "        print(f\"Perplexity for {split}: {perplexity}\")\n",
    "\n",
    "    self.train()  # turn-on training mode-\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Sat Jul 22 23:23:46 2023\n",
      "Device: cuda\n",
      "params 1115739\n",
      "Perplexity for train: 451.3076477050781\n",
      "Perplexity for eval: 451.9237365722656\n",
      "iter 0:, time = 41.77569794654846, train 6.112149238586426, val 6.113512992858887\n",
      "Perplexity for train: 4.6118597984313965\n",
      "Perplexity for eval: 5.1837005615234375\n",
      "iter 1000:, time = 145.29064178466797, train 1.5286312103271484, val 1.6455191373825073\n",
      "Perplexity for train: 3.7546401023864746\n",
      "Perplexity for eval: 4.574649810791016\n",
      "iter 2000:, time = 251.60797309875488, train 1.3229925632476807, val 1.520530104637146\n",
      "Perplexity for train: 3.329393148422241\n",
      "Perplexity for eval: 4.434844970703125\n",
      "iter 3000:, time = 356.81307220458984, train 1.2027900218963623, val 1.489492654800415\n",
      "\n",
      "Manufactured-housing story about \n",
      "one my great errossends will likel changes their come a record \n",
      "on time - migarges are the naurance business, but never by 45%,000 our lawn best selections disting future acquisitions that viewed his stock a long of many others, and investment increased its joy to rover than try. Amont the two once todrend of the \n",
      "companies from at the basis. I most this statistes that said formather I traile acquisitions more \n",
      "under after-traded. If the insurance, there(inful and committees, includential earnings and communicans subure, but is the view, as they are tucesses with it a starts, which are they \n",
      "added many years. When Charlie and I are made the llarge old hit her. Somebving of their jobs, they are truly seame. It allowance, therefore, and the employ to cause of it, \"Dann't summary profits.\" And we can't edjoyed your bank one. \n",
      "\n",
      "It know that a meaningful to commit many our more truly publicly-availed-inctedual net \n",
      "operating sector every laby life with the \n",
      "Elapsed Time: 445.3821351528168\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print(f\"Start: {time.ctime()}\")\n",
    "\n",
    "with open('./Transformer/WarrenBuffet.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\n",
    "#        'lazy dog and a quick brown fox.\\n' \\\n",
    "#        'the dog is lazy and the fox jumps quickly.\\n' \\\n",
    "#        'a fox jumps over the dog because he is lazy.\\n' \\\n",
    "#        'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'\n",
    "\n",
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model = model.to(model.device)\n",
    "model.prep(text)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "input_batch, output_batch = model.get_batch(split='train')\n",
    "_, _ = model(input_batch, output_batch)\n",
    "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
    "                                                        dtype=torch.long,\n",
    "                                                        device=model.device),\n",
    "                         max_new_tokens=1000)\n",
    "print(outputs)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Elapsed Time: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can see in the final iterations of the training that validation gets down to a perplexity of 4.43. Compared to the best model (bigram model) getting a score of about 12 in the previous assignment, we can see this as a significant improvement. Compared to the relative frequency model getting a perplexity of ~7000 (probaby as it was a character model), we can see this as an enormous improvement. It is also worth noting the significant decrease in perplexity from the first iterations to the last in the training cycle.\n",
    "\n",
    "2. Below we can see some additional output from generating 1000 more tokens. While the text does not initially seem impressive when read by a human, when we compare to our previous character based model, which had entirely illegible text with \"words\" that didn't even resemble real words, this is a drastic difference. At a glance, the text generated by this model looks like English, only upon further scrutiny does one notice that there are many spelling, grammar, and logic issues. While this does not make coherent sentences, there are coherent phrases, particularly if you can mentally correct some of the spelling issues or consider them typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "5 \n",
      "\n",
      "\n",
      "Deferred tax, decision enjoy the cisell of importance and includentially hundred the outstence weeken and a serve though our float were bought that it the everyolder of descoures, it's olders poursation of blinally run by the SEICRs signed resulable. We would notically has been bailed on by oving each ban. Just a few case for the and our cheeptentees.) \n",
      "\n",
      "Underwriting Profit Yearend Nebraska Furnings, but that it substantial. Sice knew here never hapment related itsition. So \n",
      "months) call, 12/30; state with them. (All purchased each of convinced continue to proxy a fully-value. United, resid the stribute of sthat we asked able prices. \n",
      "\n",
      "Next a good many obinable, a renter will be since 10 confill, both or \n",
      "Wall, Stanorial sums for a Berkshire-ze-pre-tax. ZShill are Stain. One met than quarters. In aggregate, it's only a distributed. When, \n",
      "I records have learned in things. Walter buy hope and se of our operating the company's problem would be the eistable, many thousands of start\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
    "                                                        dtype=torch.long,\n",
    "                                                        device=model.device),\n",
    "                         max_new_tokens=1000)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
